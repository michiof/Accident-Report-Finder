#!/usr/bin/env python3
import streamlit as st
import pandas as pd
from scipy import spatial
import chardet
import os
from dotenv import load_dotenv
import openai
import ast
import tiktoken
import datetime

# .envã‹ã‚‰APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿
load_dotenv('.env')
openai.api_key = os.environ.get("OPENAI_API_KEY")

# OpenAIã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
EMBEDDING_MODEL = "text-embedding-ada-002"
GPT_MODEL = "gpt-3.5-turbo"

# ãƒ‡ãƒ¼ã‚¿PATH
filepath_emb = './data/emb.csv'
dir_log = './chatlog'

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹
def make_message(hiyari_hatto, hiyari_hatto_emb, related_data_df):
    related_data = get_relevant_data(hiyari_hatto_emb, related_data_df)
    messages = [
        {"role": "system", "content": 'é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’èª­ã¿å–ã£ã¦ã€ãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆã¨é¡ä¼¼ã—ã¦ã„ã‚‹äº‹æ•…ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚'},
        {"role": "user", "content": f'''
            ä»¥ä¸‹ã®ãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆã¨é¡ä¼¼ã—ãŸäº‹æ•…ã‚’é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰3ã¤è¦‹ã¤ã‘å‡ºã—ã€ä»¥ä¸‹ã«æŒ‡å®šã™ã‚‹å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n
            {hiyari_hatto}\n\n
            é–¢é€£ãƒ‡ãƒ¼ã‚¿ï¼š\n
            {related_data}\n\n
            å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼‰:\n
            é¡ä¼¼äº‹æ•…ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚\n\n
            <i>ä»¶ç›®\n\n
            äº‹æ•…ã®æ¦‚è¦ï¼š\n<æ¦‚è¦>\n\n
            äº‹æ•…ã®åŸå› ï¼š\n<åŸå› >\n\n
            å ±å‘Šæ›¸ã®URL: \n<URL>\n\n
        '''},
    ]
    return messages

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹
def num_tokens(text: str, model: str) -> int:
    encoding = tiktoken.encoding_for_model(GPT_MODEL)
    return len(encoding.encode(text))

# é¡ä¼¼ãƒ™ã‚¯ãƒˆãƒ«ã®æ¤œç´¢ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰ï¼šdfã§è¿”ã™ <- ã“ã“ã‹ã‚‰
def get_relevant_data(query_embedding, related_data_df, top_k=10):
    token_budget = 4096 - 1500 #é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™å€¤ã®è¨­å®š
    relevant_data = ""

    related_data_df["Embedding"] = related_data_df["Embedding"].apply(ast.literal_eval) #ãƒªã‚¹ãƒˆã«å¤‰æ›
    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y)
    strings_and_relatednesses = [
        (row, relatedness_fn(query_embedding, row["Embedding"]))
        for i, row in related_data_df.iterrows()
    ]
    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)
    results = [row.drop("Embedding") for row, _ in strings_and_relatednesses][:top_k] # "row.drop("Embedding"), _" : Embeddingåˆ—ã‚’é™¤å¤–, relatednessesã‚’é™¤å¤–
    
    counter = 1
    for item in results:
        next_relevant_data = (
            f'\n\né–¢é€£ãƒ‡ãƒ¼ã‚¿ï¼ˆ{counter}ï¼‰:\n'
            f'äº‹æ•…ã®åç§°: {item["äº‹æ•…ç­‰å"]}\n'
            f'URL: {item["å ±å‘Šæ›¸ï¼ˆPDFï¼‰å…¬è¡¨"]}\n'
            f'æ¦‚è¦: {item["æ¦‚è¦"]}\n'
            f'åŸå› : {item["åŸå› "]}\n'
        )
        if num_tokens(relevant_data + next_relevant_data, model=GPT_MODEL) > token_budget:
            break
        else:
            relevant_data += next_relevant_data
            counter += 1
    
    return relevant_data

# Embeddingsã®è¨ˆç®—
def cal_embedding(hiyari_hatto, model=EMBEDDING_MODEL):
    try:
        embedding = openai.Embedding.create(input=hiyari_hatto, model=model)["data"][0]["embedding"]
        return embedding
    except Exception as e:
        st.warning(f"Embeddingsè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„: {e}")
        return False

# æ¤œç´¢ç”»é¢ã§ã®å‡¦ç†
def chat_page(related_data_df):
    new_msg = st.text_input('ãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
    
    if st.button('æ¤œç´¢'):
        if not os.path.exists(filepath_emb):
            st.warning('å‚ç…§ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿æº–å‚™ç”»é¢ã§è¨­å®šã—ã¦ãã ã•ã„ã€‚')
            return
        
        if new_msg:
            with st.spinner('æ¤œç´¢ä¸­...'):
                hiyari_hatto = f'ãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆ: {new_msg}'
                hiyari_hatto_emb = cal_embedding(new_msg)
                CHAT_INPUT_MESSAGES = make_message(hiyari_hatto, hiyari_hatto_emb, related_data_df)
            
            # æ–‡ç« ç”Ÿæˆä¸­ã¯temp_placeholderã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¡¨ç¤º
            try:
                with st.spinner('æ–‡ç« ç”Ÿæˆä¸­...'):
                    response_all = ""
                    temp_placeholder = st.empty()
                    response = openai.ChatCompletion.create(model=GPT_MODEL,messages=CHAT_INPUT_MESSAGES, temperature=0.0, stream=True)
                    for chunk in response:
                        response_delta = chunk['choices'][0]['delta'].get('content', '')
                        response_all += response_delta
                        temp_placeholder.write(response_all)
                st.session_state.messages.append('---')
                st.session_state.messages.append(response_all)
                st.session_state.messages.append(hiyari_hatto)
                temp_placeholder.empty() #Streaméƒ¨åˆ†ã®éè¡¨ç¤º
                st.empty()
            except Exception as e:
                st.error(f"å†åº¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    st.write('---')
    st.write('æ¤œç´¢å±¥æ­´:')
    for message in reversed(st.session_state.messages):
        st.write(message)

    # ãƒãƒ£ãƒƒãƒˆã®å±¥æ­´ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    if st.button('æ¤œç´¢çµæœã‚’ä¿å­˜'):
        current_time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        file_name = f"{dir_log}/chat_history_{current_time}.txt"
        with open(file_name, 'w', encoding='utf-8') as file:
            for message in st.session_state.messages:
                file.write(message + '\n')
        
        st.success(f'æ¤œç´¢çµæœãŒ {file_name} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚')

    # å±¥æ­´ã®ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.button('å±¥æ­´ã®ã‚¯ãƒªã‚¢'):
        st.session_state.messages = []
        st.empty()

# ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª¿ã¹ã‚‹é–¢æ•°
def detect_file_encoding(file_content):
    result = chardet.detect(file_content)
    return result['encoding']

# ãƒ‡ãƒ¼ã‚¿æº–å‚™ç”»é¢
def csv_import_page(reload=False):
    uploaded_file = st.file_uploader("CSV/TSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", type=["csv", "tsv"])
    if uploaded_file:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ¤œå‡º
        detected_encoding = detect_file_encoding(uploaded_file.read())
        # ä¸€åº¦ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’èª­ã‚€ã¨ã€å†åº¦èª­ã‚€ãŸã‚ã«ã¯ãƒã‚¤ãƒ³ã‚¿ã‚’å…ˆé ­ã«æˆ»ã™å¿…è¦ãŒã‚ã‚‹
        uploaded_file.seek(0)
        
        try:
            df = pd.read_csv(uploaded_file, encoding=detected_encoding, delimiter='\t', on_bad_lines='skip', nrows=10)
            st.write(f'èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°: {len(df)} (ä¸Šé™1,000)')
            emb_row = None
            
            # ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢æ–¹æ³•ã‚’é¸æŠ
            search_method = st.radio("æ¤œç´¢æ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„:", ("åŸå› ã®é¡ä¼¼æ€§ã‚’æ¤œç´¢", "æ¦‚è¦ã®é¡ä¼¼æ€§ã‚’æ¤œç´¢"))

            if search_method == "åŸå› ã®é¡ä¼¼æ€§ã‚’æ¤œç´¢":
                emb_row = 'åŸå› '
                if 'åŸå› ' in df.columns:
                    emb_row = 'åŸå› '
                else:
                    st.warning("èª­ã¿è¾¼ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«ã« 'æ¦‚è¦' ã¨ã„ã†ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            elif search_method == "æ¦‚è¦ã®é¡ä¼¼æ€§ã‚’æ¤œç´¢":
                if 'æ¦‚è¦' in df.columns:
                    emb_row = 'æ¦‚è¦'
                else:
                    st.warning("èª­ã¿è¾¼ã‚“ã ãƒ•ã‚¡ã‚¤ãƒ«ã« 'æ¦‚è¦' ã¨ã„ã†ã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            
            if emb_row and st.button("ãƒ™ã‚¯ãƒˆãƒ«è¨ˆç®—ã‚’å®Ÿè¡Œ"):
                st.write('Embeddingsã®è¨ˆç®—ä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„...')

                # é€²è¡ŒçŠ¶æ³ãƒãƒ¼ã®åˆæœŸåŒ–
                progress_bar = st.progress(0)
                total_rows = len(df)

                # Embeddingsã‚’è¨ˆç®—ã—ã¦ä¿å­˜
                embeddings = []
                for idx, row in df.iterrows():
                    embedding = cal_embedding(row[emb_row])
                    embeddings.append(embedding)
                    
                    # é€²è¡ŒçŠ¶æ³ã®æ›´æ–°
                    progress_bar.progress((idx + 1) / total_rows)

                df['Embedding'] = embeddings

                # EmbeddingsãŒå«ã¾ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’output.csvã«å‡ºåŠ›
                df.to_csv(filepath_emb, index=False)
                st.success('Embeddingã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚æ¤œç´¢ãŒå¯èƒ½ã§ã™ã€‚')
                
                if reload:
                    if st.button('ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿'):
                        st.experimental_rerun()
                        
        except (UnicodeDecodeError, pd.errors.ParserError) as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")

def main():
    st.title('ğŸ” Accident Report Finder')
    st.write('å…¥åŠ›ã—ãŸãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆã¨é¡ä¼¼ã™ã‚‹éå»ã®äº‹æ•…ã‚’æ¤œç´¢ã§ãã¾ã™ã€‚')

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’ç¶­æŒã™ã‚‹
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‹ã®ç¢ºèª
    if not os.path.exists(filepath_emb):
        dir_name = os.path.dirname(filepath_emb)
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)
        st.warning('å‚ç…§ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚')
        csv_import_page(reload=True)
    else:
        related_data_df = pd.read_csv(filepath_emb)
        os.makedirs(dir_log, exist_ok=True) #Logãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã¯ä½œæˆ
        tab_chat, tab_import_file = st.tabs(["æ¤œç´¢ç”»é¢", "ãƒ‡ãƒ¼ã‚¿æº–å‚™ç”»é¢"])
        with tab_chat:
            chat_page(related_data_df)
        with tab_import_file:
            csv_import_page()

if __name__ == '__main__':
    main()
