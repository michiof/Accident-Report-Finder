# For Streamlit online app

import streamlit as st
import pandas as pd
from openai import OpenAI
import pinecone
import tiktoken

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"],)

# OpenAIã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
EMBEDDING_MODEL = "text-embedding-ada-002"
GPT_MODEL = "gpt-3.5-turbo"

# Streamlit setup
st.set_page_config(
   page_title="Accident Report Finder",
   page_icon="ğŸ”",
   menu_items={
        'About': "**Accident Report Finder** v1.2.0 made by [Michio Fujii](https://github.com/michiof)",
    }
)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹
def make_message(user_input, user_input_emb, num_of_output, language):
    related_data = get_relevant_data(user_input_emb)
    if language == "ja":
        messages = [
            {"role": "system", "content": "é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’èª­ã¿å–ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ã¨åŸå› ãŒé¡ä¼¼ã—ã¦ã„ã‚‹äº‹æ•…ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚"},
            {"role": "user", "content": f"""
                ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ã¨äº‹æ•…ã®åŸå› ãŒé¡ä¼¼ã—ãŸäº‹æ•…ã‚’é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰{num_of_output}ä»¶è¦‹ã¤ã‘å‡ºã—ã€ä»¥ä¸‹ã«æŒ‡å®šã™ã‚‹å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n
                ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ï¼š\n
                {user_input}\n\n
                é–¢é€£ãƒ‡ãƒ¼ã‚¿ï¼š\n
                {related_data}\n\n
                å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆMarkdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼‰:\n
                é¡ä¼¼äº‹æ•…ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚\n\n
                iä»¶ç›®\n\n
                äº‹æ•…ã®åç§°ï¼š\n<Title>\n\n
                äº‹æ•…ã®ç¨®é¡: \n<Type_accident>\n\n
                äº‹æ•…ã®æ¦‚è¦ï¼š\n<Outline>\n\n
                äº‹æ•…ã®åŸå› ï¼š\n<Cause>\n\n
                å ±å‘Šæ›¸ã®URL: \n<URL>\n\n
            """},
        ]
    else:
        messages = [
            {"role": "system", "content": "Read the content of the relevant data and find accidents that are similar to the cause of the accident entered by the user."},
            {"role": "user", "content": f"""
                Find {num_of_output} accidents from the related data that are similar cause to the following user input, and output them in the specified output format below.\n
                If related data is written in Japanese, please trnslate them to English and output.\n\n
                Cause of the accident entered by the user:\n
                {user_input}\n\n
                Related data:\n
                {related_data}\n\n
                Output format (Please output in Markdown format):\n
                The similar accidents are as follows.\n\n
                **No. i:**\n\n
                **Name of accident:**  <Title> (translate in English and output)\n\n
                **Type of accident:**  <Type_accident> (translate in English and output)\n\n
                **Outline of accident:**  <Outline> (translate in English and output)\n\n
                **Cuase of accident:**  <Cause> (translate in English and output)\n\n
                **Report URL:**\n<URL>\n\n
            """},
        ]
    return messages

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹
def num_tokens(text: str, model: str) -> int:
    encoding = tiktoken.encoding_for_model(GPT_MODEL)
    return len(encoding.encode(text))

# Pineconeã®metadataã‚’å–å¾—ã™ã‚‹
def get_metadata(match):
    info = []
    for key, value in match["metadata"].items():
        info.append(f"{key}: {value}\n")
    return "\n".join(info)

# Multiselectionã®å€¤ã‹ã‚‰Pineconeç”¨ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½œæˆã—ã€session_stateã«ä¿å­˜ã™ã‚‹
def make_pinecone_filter(filter_selection):
    filter = []
    if "Severity_2" in filter_selection:
        filter.append({"Severity": {"$in": ["2"]}})
    if "Cat3" in filter_selection:
        filter.append({"Cat_GrossTon": {"$in": ["Cat3"]}})
    
    if len(filter) > 1:
        filter_dic_for_pinecone = {"$and": filter}
    elif filter:
        filter_dic_for_pinecone = filter[0]
    else:
        filter_dic_for_pinecone = {}
    st.session_state['filter_dic'] = filter_dic_for_pinecone

# é¡ä¼¼ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
def get_relevant_data(query_embedding, top_k=20):
    pinecone.init(api_key=st.secrets["PINECONE_API_KEY"], environment=st.secrets["PINECONE_ENVIRONMENT"])
    pinecone_index = pinecone.Index(st.secrets["PINECONE_INDEX"])
    token_budget = 4096 - 1500 #é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™å€¤ã®è¨­å®š
    relevant_data = ""
    filter_dic = st.session_state['filter_dic']
    for _ in range(3): # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯3å›ã¾ã§ãƒˆãƒ©ã‚¤ã™ã‚‹
        try:
            results = pinecone_index.query(
                            vector=query_embedding,
                            filter=filter_dic,
                            top_k=top_k, 
                            include_metadata=True
                        )
            for i, match in enumerate(results["matches"], start=1):
                metadata = get_metadata(match)
                next_relevant_data = f"\n\nRelevant data {i}:\n{metadata}"
                if (
                    num_tokens(relevant_data + next_relevant_data, model=GPT_MODEL)
                    > token_budget
                ):
                    break
                else:
                    relevant_data += next_relevant_data
            return relevant_data
        except Exception as e:
            print(f"Error: {str(e)}")
            continue
    raise Exception("Error: Failed to retrieve relevant data after 3 attempts")

# Embeddingsã®è¨ˆç®—
def cal_embedding(user_input, model=EMBEDDING_MODEL):
    for _ in range(3): # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯3å›ã¾ã§ãƒˆãƒ©ã‚¤ã™ã‚‹
        try:
            return client.embeddings.create(input=user_input, model=model).data[0].embedding
        except Exception as e:
            print(f"Error: {str(e)}")
            continue
    raise Exception("Failed to calculate embedding after 3 attempts")

# æ¤œç´¢ç”»é¢ã§ã®å‡¦ç†
def chat_page(num_of_output, language):
    if language == "ja":
        label_msg_text_area = "æ¤œç´¢ã—ãŸã„äº‹æ•…åŸå› ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:"
        placeholder_text_area = "æ½®æµãŒå¼·ãèˆµãŒåŠ¹ã‹ãªããªã£ãŸã€‚"
        lable_load_sample = "ã‚µãƒ³ãƒ—ãƒ«è³ªå•ã‚’æŒ¿å…¥"
        label_results = "### æ¤œç´¢å±¥æ­´: "
        label_search_botton = "æ¤œç´¢"
        label_reset_button = "ãƒªã‚»ãƒƒãƒˆ"
        label_save_button = "æ¤œç´¢çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
        msg_header_search_text = "æ¤œç´¢ã™ã‚‹åŸå› : "
        msg_while_searching = "æ¤œç´¢ä¸­..."
        msg_gen_result = "æ¤œç´¢çµæœã‚’å‡ºåŠ›ä¸­..."
        error_message = "å†åº¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
    else:
        label_msg_text_area = "Please enter the cause of the accident you want to search for:"
        placeholder_text_area = "The tidal current was strong, and the rudder became ineffective."
        lable_load_sample = "Input a sample"
        label_results = "### Results: "
        label_search_botton = "Search"
        label_reset_button = "Reset"
        label_save_button = "Download search results."
        msg_header_search_text = "Cause to search for: "
        msg_while_searching = "Searching..."
        msg_gen_result = "Outputting search results..."
        error_message = "Please search again. An error has occurred."

    new_msg = st.text_input(label_msg_text_area, value=st.session_state.sample_question, placeholder=placeholder_text_area)
    if st.button(lable_load_sample):
        st.session_state.sample_question = placeholder_text_area # load a sample question
    if st.button(label_search_botton):
        if new_msg:
            try:
                with st.spinner(msg_while_searching):
                    user_input = f"{msg_header_search_text}{new_msg}"
                    user_input_emb = cal_embedding(new_msg)
                    CHAT_INPUT_MESSAGES = make_message(user_input, user_input_emb, num_of_output, language)
                with st.spinner(msg_gen_result):
                    response_all = ""
                    temp_placeholder = st.empty()
                    stream = client.chat.completions.create(model=GPT_MODEL,messages=CHAT_INPUT_MESSAGES, temperature=0.0, stream=True)
                    for part in stream:
                        response_delta = (part.choices[0].delta.content or "")
                        response_all += response_delta
                        temp_placeholder.write(response_all)
                st.session_state.messages.append("---")
                st.session_state.messages.append(response_all)
                st.session_state.messages.append(user_input)
                temp_placeholder.empty() #Streaméƒ¨åˆ†ã®éè¡¨ç¤º
                st.empty()
            except Exception as e:
                print(str(e))
                st.error(error_message)

    st.write("---")
    st.write(label_results)
    output_messages = ""
    for message in reversed(st.session_state.messages):
        st.write(message)
        output_messages += message + "\n\n"

    # å±¥æ­´ã®ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.button(label_reset_button):
        st.session_state.messages = []
        st.empty()
    
    st.download_button(label_save_button, output_messages)

def main():
    st.title("ğŸ” Accident Report Finder")
    language_selection = st.radio("Language", ("English", "æ—¥æœ¬èª"), horizontal=True)

    if language_selection == "æ—¥æœ¬èª":
        language = "ja"
        caption_1 = "åŸå› ã‚„ãƒ’ãƒ¤ãƒªãƒãƒƒãƒˆã‚’å…¥åŠ›ã™ã‚‹ã¨ã€éå»ã®é¡ä¼¼èˆ¹èˆ¶äº‹æ•…ã‚’æ¤œç´¢ã§ãã¾ã™ã€‚"
        caption_2 = "ãƒ‡ãƒ¼ã‚¿å‡ºå…¸: [é‹è¼¸å®‰å…¨å§”å“¡ä¼š](https://jtsb.mlit.go.jp/jtsb/ship/index.php)ãŒå…¬é–‹ã—ã¦ã„ã‚‹15,334ä»¶(2023å¹´12æœˆ1æ—¥æ™‚ç‚¹)ã®èˆ¹èˆ¶äº‹æ•…å ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’æœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç”¨ã«åŠ å·¥ã—ã¦åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚"
        label_num_of_output = "æœ€å¤§æ¤œç´¢ä»¶æ•°"
        label_filter_title = "ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"
        label_filter_1 = "è»½å¾®ãªã‚‚ã®ã‚’é™¤ã"
        label_filter_2 = "å°å‹èˆ¹èˆ¶ã‚’é™¤ã"
    else:
        language = "en"
        caption_1 = "You can search for past similar ship accidents by entering the cause or near-miss incidents."
        caption_2 = "Data source: The program uses processed data from 15,334 ship accident reports (as of December 1, 2023) publicly available from the [Japan Transport Safety Board](https://jtsb.mlit.go.jp/jtsb/ship/index.php)."
        label_num_of_output = "Max. search results"
        label_filter_title = "Eexclude:"
        label_filter_1 = "Minor accident"
        label_filter_2 = "Small craft"
    st.caption(caption_1)
    st.caption(caption_2)
    st.write("---")
    st.sidebar.title("Accident Report Finder")
    with st.sidebar:
        st.write("Version: 1.2.0")
        st.write("Made by [Michio Fujii](https://github.com/michiof)")
        st.write("---")
        
        num_of_output = st.slider(label_num_of_output, 1, 5, 3)
        filter_selection = st.multiselect(label_filter_title,
                                [label_filter_1, label_filter_2],
                                [label_filter_1, label_filter_2]
                            )
        converted_filter_selection = [text.replace(label_filter_1,"Severity_2").replace(label_filter_2, "Cat3") for text in filter_selection]
        make_pinecone_filter(converted_filter_selection)

    if "messages" not in st.session_state:
        st.session_state['messages']= []
    if 'sample_question' not in st.session_state:
        st.session_state['sample_question'] = ""

    chat_page(num_of_output, language)

if __name__ == "__main__":
    main()
