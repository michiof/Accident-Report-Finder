# For Streamlit online app

import streamlit as st
import pandas as pd
from openai import OpenAI
import pinecone
import tiktoken

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"],)

# OpenAIã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
EMBEDDING_MODEL = "text-embedding-ada-002"
GPT_MODEL = "gpt-3.5-turbo"

# Streamlit setup
st.set_page_config(
   page_title="Accident Report Finder",
   page_icon="ğŸ”",
)

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹
def make_message(user_input, user_input_emb, num_of_output):
    related_data = get_relevant_data(user_input_emb)
    messages = [
        {"role": "system", "content": "é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’èª­ã¿å–ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ã¨åŸå› ãŒé¡ä¼¼ã—ã¦ã„ã‚‹äº‹æ•…ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚"},
        {"role": "user", "content": f"""
            ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ã¨äº‹æ•…ã®åŸå› ãŒé¡ä¼¼ã—ãŸäº‹æ•…ã‚’é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰{num_of_output}ä»¶è¦‹ã¤ã‘å‡ºã—ã€ä»¥ä¸‹ã«æŒ‡å®šã™ã‚‹å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ï¼š\n
            {user_input}\n\n
            é–¢é€£ãƒ‡ãƒ¼ã‚¿ï¼š\n
            {related_data}\n\n
            å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆMarkdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼‰:\n
            é¡ä¼¼äº‹æ•…ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚\n\n
            iä»¶ç›®\n\n
            äº‹æ•…ã®åç§°ï¼š\n<Title>\n\n
            äº‹æ•…ã®ç¨®é¡: \n<Type_accident>\n\n
            äº‹æ•…ã®æ¦‚è¦ï¼š\n<Outline>\n\n
            äº‹æ•…ã®åŸå› ï¼š\n<Cause>\n\n
            å ±å‘Šæ›¸ã®URL: \n<URL>\n\n
        """},
    ]
    return messages

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹
def num_tokens(text: str, model: str) -> int:
    encoding = tiktoken.encoding_for_model(GPT_MODEL)
    return len(encoding.encode(text))

# Pineconeã®metadataã‚’å–å¾—ã™ã‚‹
def get_metadata(match):
    info = []
    for key, value in match["metadata"].items():
        info.append(f"{key}: {value}\n")
    return "\n".join(info)

# Multiselectionã®å€¤ã‹ã‚‰Pineconeç”¨ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½œæˆã—ã€session_stateã«ä¿å­˜ã™ã‚‹
def make_pinecone_filter(filter_selection):
    filter = []
    if 'è»½å¾®ãªã‚‚ã®ã‚’é™¤ã' in filter_selection:
        filter.append({"Severity": {"$in": ["2"]}})
    if 'å°å‹èˆ¹èˆ¶ã‚’é™¤ã' in filter_selection:
        filter.append({"Cat_GrossTon": {"$in": ["Cat3"]}})
    
    if len(filter) > 1:
        filter_dic_for_pinecone = {"$and": filter}
    elif filter:
        filter_dic_for_pinecone = filter[0]
    else:
        filter_dic_for_pinecone = {}
    st.session_state['filter_dic'] = filter_dic_for_pinecone

# é¡ä¼¼ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
def get_relevant_data(query_embedding, top_k=20):
    pinecone.init(api_key=st.secrets["PINECONE_API_KEY"], environment=st.secrets["PINECONE_ENVIRONMENT"])
    pinecone_index = pinecone.Index(st.secrets["PINECONE_INDEX"])
    token_budget = 4096 - 1500 #é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™å€¤ã®è¨­å®š
    relevant_data = ""
    filter_dic = st.session_state['filter_dic']
    for _ in range(3): # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯3å›ã¾ã§ãƒˆãƒ©ã‚¤ã™ã‚‹
        try:
            results = pinecone_index.query(
                            vector=query_embedding,
                            filter=filter_dic,
                            top_k=top_k, 
                            include_metadata=True
                        )
            for i, match in enumerate(results["matches"], start=1):
                metadata = get_metadata(match)
                next_relevant_data = f"\n\nRelevant data {i}:\n{metadata}"
                if (
                    num_tokens(relevant_data + next_relevant_data, model=GPT_MODEL)
                    > token_budget
                ):
                    break
                else:
                    relevant_data += next_relevant_data
            return relevant_data
        except Exception as e:
            print(f"Error: {str(e)}")
            continue
    raise Exception("Error: Failed to retrieve relevant data after 3 attempts")

# Embeddingsã®è¨ˆç®—
def cal_embedding(user_input, model=EMBEDDING_MODEL):
    for _ in range(3): # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯3å›ã¾ã§ãƒˆãƒ©ã‚¤ã™ã‚‹
        try:
            return client.embeddings.create(input=user_input, model=model).data[0].embedding
        except Exception as e:
            print(f"Error: {str(e)}")
            continue
    raise Exception("Failed to calculate embedding after 3 attempts")

# æ¤œç´¢ç”»é¢ã§ã®å‡¦ç†
def chat_page(num_of_output):
    new_msg = st.text_area("æ¤œç´¢ã—ãŸã„äº‹æ•…åŸå› ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", placeholder="æ½®æµãŒå¼·ãèˆµãŒåŠ¹ã‹ãªããªã£ãŸã€‚")
    
    if st.button("æ¤œç´¢"):
        if new_msg:
            try:
                with st.spinner("æ¤œç´¢ä¸­..."):
                    user_input = f"æ¤œç´¢ã™ã‚‹åŸå› : {new_msg}"
                    user_input_emb = cal_embedding(new_msg)
                    CHAT_INPUT_MESSAGES = make_message(user_input, user_input_emb, num_of_output)
                with st.spinner("æ–‡ç« ç”Ÿæˆä¸­..."):
                    response_all = ""
                    temp_placeholder = st.empty()
                    stream = client.chat.completions.create(model=GPT_MODEL,messages=CHAT_INPUT_MESSAGES, temperature=0.0, stream=True)
                    for part in stream:
                        response_delta = (part.choices[0].delta.content or "")
                        response_all += response_delta
                        temp_placeholder.write(response_all)
                st.session_state.messages.append("---")
                st.session_state.messages.append(response_all)
                st.session_state.messages.append(user_input)
                temp_placeholder.empty() #Streaméƒ¨åˆ†ã®éè¡¨ç¤º
                st.empty()
            except Exception as e:
                print(str(e))
                st.error(f"å†åº¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")

    st.write("---")
    st.write("æ¤œç´¢å±¥æ­´:")
    output_messages = ""
    for message in reversed(st.session_state.messages):
        st.write(message)
        output_messages += message + "\n\n"

    # å±¥æ­´ã®ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.button("å±¥æ­´ã‚’æ¶ˆå»"):
        st.session_state.messages = []
        st.empty()
    
    st.download_button("æ¤œç´¢çµæœã‚’ä¿å­˜", output_messages)

def main():
    st.title("ğŸ” Accident Report Finder")
    st.caption("å…¥åŠ›ã—ãŸäº‹æ•…åŸå› ã¨é¡ä¼¼ã™ã‚‹éå»ã®èˆ¹èˆ¶äº‹æ•…ã‚’æ¤œç´¢ã§ãã¾ã™ã€‚")
    st.caption("ãƒ‡ãƒ¼ã‚¿å‡ºå…¸: [é‹è¼¸å®‰å…¨å§”å“¡ä¼š](https://jtsb.mlit.go.jp/jtsb/ship/index.php)ãŒå…¬é–‹ã—ã¦ã„ã‚‹15,334ä»¶(2023å¹´12æœˆ1æ—¥æ™‚ç‚¹)ã®èˆ¹èˆ¶äº‹æ•…å ±å‘Šæ›¸ãƒ‡ãƒ¼ã‚¿ã‚’æœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ç”¨ã«åŠ å·¥ã—ã¦åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚")
    st.write("---")
    st.sidebar.title("Accident Report Finder")
    with st.sidebar:
        st.write("Version: 1.1.0")
        st.write("Made by [Michio Fujii](https://github.com/michiof)")
        st.write("---")
        
        num_of_output = st.slider("æœ€å¤§æ¤œç´¢ä»¶æ•°", 1, 5, 3)

        # filterè¨­å®š
        filter_selection = st.multiselect("ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼",
                                ['è»½å¾®ãªã‚‚ã®ã‚’é™¤ã', 'å°å‹èˆ¹èˆ¶ã‚’é™¤ã'],
                                ['è»½å¾®ãªã‚‚ã®ã‚’é™¤ã', 'å°å‹èˆ¹èˆ¶ã‚’é™¤ã']
                            )
        make_pinecone_filter(filter_selection)

    if "messages" not in st.session_state:
        st.session_state.messages = []
    chat_page(num_of_output)

if __name__ == "__main__":
    main()
