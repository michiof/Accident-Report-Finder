# For Streamlit online app
import streamlit as st
import pandas as pd
import openai
import pinecone
import tiktoken

openai.api_key = st.secrets["OPENAI_API_KEY"]
pinecone.init(api_key=st.secrets["PINECONE_API_KEY"], environment=st.secrets["PINECONE_ENVIRONMENT"])
pinecone_index = pinecone.Index(st.secrets["PINECONE_INDEX"])

# OpenAIã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
EMBEDDING_MODEL = "text-embedding-ada-002"
GPT_MODEL = "gpt-3.5-turbo"

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹
def make_message(user_input, user_input_emb):
    related_data = get_relevant_data(user_input_emb)
    messages = [
        {"role": "system", "content": 'é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’èª­ã¿å–ã£ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ã¨åŸå› ãŒé¡ä¼¼ã—ã¦ã„ã‚‹äº‹æ•…ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚'},
        {"role": "user", "content": f'''
            ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ã¨äº‹æ•…ã®åŸå› ãŒé¡ä¼¼ã—ãŸäº‹æ•…ã‚’é–¢é€£ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰3ã¤è¦‹ã¤ã‘å‡ºã—ã€ä»¥ä¸‹ã«æŒ‡å®šã™ã‚‹å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\n
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸäº‹æ•…åŸå› ï¼š\n
            {user_input}\n\n
            é–¢é€£ãƒ‡ãƒ¼ã‚¿ï¼š\n
            {related_data}\n\n
            å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆMarkdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼‰:\n
            é¡ä¼¼äº‹æ•…ã¯ä»¥ä¸‹ã®ã¨ãŠã‚Šã§ã™ã€‚\n\n
            iä»¶ç›®\n\n
            äº‹æ•…ã®åç§°ï¼š\n<Title>\n\n
            äº‹æ•…ã®ç¨®é¡: \n<Type_accident>\n\n
            äº‹æ•…ã®æ¦‚è¦ï¼š\n<Outline>\n\n
            äº‹æ•…ã®åŸå› ï¼š\n<Cause>\n\n
            å ±å‘Šæ›¸ã®URL: \n<URL>\n\n
        '''},
    ]
    return messages

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—ã™ã‚‹
def num_tokens(text: str, model: str) -> int:
    encoding = tiktoken.encoding_for_model(GPT_MODEL)
    return len(encoding.encode(text))

# Pineconeã®metadataã‚’å–å¾—ã™ã‚‹
def get_metadata(match):
    info = []
    for key, value in match["metadata"].items():
        info.append(f'{key}: {value}\n')
    return '\n'.join(info)

# é¡ä¼¼ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
def get_relevant_data(query_embedding, top_k=10):
    token_budget = 4096 - 1500 #é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™å€¤ã®è¨­å®š
    relevant_data = ""
    try:
        results = pinecone_index.query(vector=query_embedding, top_k=top_k, include_metadata=True)
        for i, match in enumerate(results["matches"], start=1):
            metadata = get_metadata(match)
            next_relevant_data = f'\n\nRelevant data {i}:\n{metadata}'
            if (
                num_tokens(relevant_data + next_relevant_data, model=GPT_MODEL)
                > token_budget
            ):
                break
            else:
                relevant_data += next_relevant_data
        return relevant_data
    except Exception as e:
        st.warning(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œç´¢ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„: {e}")
        return False

# Embeddingsã®è¨ˆç®—
def cal_embedding(user_input, model=EMBEDDING_MODEL):
    try:
        embedding = openai.Embedding.create(input=user_input, model=model)["data"][0]["embedding"]
        return embedding
    except Exception as e:
        st.warning(f"Embeddingsè¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„: {e}")
        return False

# æ¤œç´¢ç”»é¢ã§ã®å‡¦ç†
def chat_page():
    new_msg = st.text_input('æ¤œç´¢ã—ãŸã„äº‹æ•…åŸå› ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:ï¼ˆä¾‹ï¼‰æ½®æµãŒå¼·ãèˆµãŒåŠ¹ã‹ãªããªã£ãŸã€‚')
    
    if st.button('æ¤œç´¢'):
        if new_msg:
            with st.spinner('æ¤œç´¢ä¸­...'):
                user_input = f'æ¤œç´¢ã™ã‚‹åŸå› : {new_msg}'
                user_input_emb = cal_embedding(new_msg)
                CHAT_INPUT_MESSAGES = make_message(user_input, user_input_emb)

            # æ–‡ç« ç”Ÿæˆä¸­ã¯temp_placeholderã«ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¡¨ç¤º
            try:
                with st.spinner('æ–‡ç« ç”Ÿæˆä¸­...'):
                    response_all = ""
                    temp_placeholder = st.empty()
                    response = openai.ChatCompletion.create(model=GPT_MODEL,messages=CHAT_INPUT_MESSAGES, temperature=0.0, stream=True)
                    for chunk in response:
                        response_delta = chunk['choices'][0]['delta'].get('content', '')
                        response_all += response_delta
                        temp_placeholder.write(response_all)
                st.session_state.messages.append('---')
                st.session_state.messages.append(response_all)
                st.session_state.messages.append(user_input)
                temp_placeholder.empty() #Streaméƒ¨åˆ†ã®éè¡¨ç¤º
                st.empty()
            except Exception as e:
                st.error(f"å†åº¦æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    st.write('---')
    st.write('æ¤œç´¢å±¥æ­´:')
    for message in reversed(st.session_state.messages):
        st.write(message)

    # å±¥æ­´ã®ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³
    if st.button('å±¥æ­´ã®ã‚¯ãƒªã‚¢'):
        st.session_state.messages = []
        st.empty()

def main():
    st.title('ğŸ” Accident Report Finder =beta=')
    st.write('å…¥åŠ›ã—ãŸæ–‡ç« ã¨åŸå› ãŒé¡ä¼¼ã™ã‚‹éå»ã®èˆ¹èˆ¶äº‹æ•…ã‚’æ¤œç´¢ã§ãã¾ã™ã€‚')
    st.write('é‹è¼¸å®‰å…¨å§”å“¡ä¼šãŒå…¬é–‹ã—ã¦ã„ã‚‹14,875ä»¶ã®èˆ¹èˆ¶äº‹æ•…å ±å‘Šæ›¸(2023å¹´6æœˆæ™‚ç‚¹)ã‚’æœ¬ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®æ¤œç´¢ç”¨ã«åŠ å·¥ã—ã¦åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚')
    st.write('å‡ºå…¸ï¼š[é‹è¼¸å®‰å…¨å§”å“¡ä¼šãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸: å ±å‘Šæ›¸æ¤œç´¢](https://jtsb.mlit.go.jp/jtsb/ship/index.php)')
    st.write('---')

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’ç¶­æŒã™ã‚‹
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    chat_page()

if __name__ == '__main__':
    main()
